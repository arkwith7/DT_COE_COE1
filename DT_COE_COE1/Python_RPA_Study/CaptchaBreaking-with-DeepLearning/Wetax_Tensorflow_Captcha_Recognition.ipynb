{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version:  2.3.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "print(\"Tensorflow version: \", tf.__version__)\n",
    "\n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 일부 이미지 시각화하기\n",
    "데이터 증가를 이해하기 위해 일부 학습용 이미지를 시각화해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images found:  1104\n",
      "Shape of image:  (62, 175, 3)\n",
      "Shape of image:  (62, 175, 3)\n",
      "Shape of image:  (62, 175, 3)\n",
      "Shape of image:  (62, 175, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASUAAACUCAYAAADVhj8TAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAA/0klEQVR4nO292XMbSXY9fGov7BsBkCC4iJvUPd3umXHMjP3i+af94Af/Yl4mxvbEuBepWxslEgS4ith3oFBV3wO/m8oqFBZSUjfUzhOhkASgsrIyb568ebeSXNeFgICAwKpA/qU7ICAgIMBDkJKAgMBKQZCSgIDASkGQkoCAwEpBkJKAgMBKQZCSgIDASkGd96UkSSJe4P8oXNeVPvU9hHz938U8+RKakoCAwEpBkJKAgMBKQZCSgIDASkGQkoCAwEphrqH71wRFUaCqKjRNg6qqGI1GsCwLk8nkg9tOpVLI5/MYDAbodDro9XoYjUYP6qNhGIhEIojH49B1HbIsYzQa4fr6Gt1u90H9kyQJqqpC13VEo1FEo1EYhgFFUXBxcYFmswnHcR7U9qpClmWoqorJZPKreDZd1wHczaUsyxiPx7Bt+8HtGYYBSZKwvb0NSZLQ7/fRbDYxHo9hWda9xoyXr1gshmg0Cl3XIUkSxuMxut0uer0ehsMhLMvConzbj05KsVgMqVQKAOC6LiTpvZFd13WcnJw8SEgURYGu61hbW/O0SWi322g2m4HXyrKM7e1tpNNpqKqKUCiE0WiEdruNdruN8XgMVVWhKAq7Jp1O4/vvv1/Yr3A4jKOjI4RCIQB3z9xoNFCtVnF5ebn084VCIWxtbWFtbQ2apnme0XEcRCIRnJ2dzXzGWUilUkgmk4jFYojH455nlGUZmUwGP/30E25vb+/V7ipBURRsbm7CdV1YlgUAiEajiEQi6Pf7qNfraLfb7LtlsL6+7pkH13WZ3LZarXttEJqmIRqNIhaLsc8mk8nS8pFIJHBwcADHcdjm2mw20W63Yds2IwBCJpPBjz/+OPN519bWcHR0BMuyEAqFPNd2u122sVLfTdPEy5cvp9pRVRWJRILJVjweh6oGU4pt2+h2u2g2mwsVgY9CSrIsIx6PI5PJIJPJQNM0jMdj9p2iKNA0DYqiQFEUvH79eiFbAncLfm9vD8lkEp1Oh2k5tENQe4qiYDweo9fr4fnz51M7SD6fRz6fh2EYAO4EzDAMZLNZZDIZOI4DSZJg2zbbYU3TxNbWFiqVysz+GYaB7e1tmKbJPpMkCaZpolAoLC10qVQKR0dHnnZ4yLKMZDIJ0zRRKpVwc3OzsE0inL29PZim6RE8GntaZNvb24ykPzf86U9/gm3bbFORJIlpE8Ddgs5kMjg7O8PV1dXcthRFQSKRwN7eHiNvGiPDMCDLMiaTCUajEZrNJk5OThbKcSqVQqFQQCQSYTKmqiocx1lKPuLxOA4ODhCPx+G6LrtfPp9HLpdjsjuZTJjshsNhSJKEd+/e4eLiwtPHaDSK3d1dmKbJtC/gvUyQpuO6LhtPWZZhmiZev36Nfr8P4E7B2N/fRyqVYuTN988PWZaRSCSQTCYXKiUfTEqGYaBQKCCfz3sY28+YpMpls1lIkoTj4+OZ6qeiKFhfX0ehUEAoFGKLchYsy4KmaUgmk/jtb3+LH3/80XN8yuVyMAyDDQYJ7mQyYTuP67qMQF3XRbfbxaNHjyDLMsrl8tRgy7KMQqGAbDY71R9ZlpfWBuPxOP74xz9iPB6zSZ1MJjBNE7ZtM6EjsiPyCuoTwTRNfPnll4hGo+xax3HYQiUNluYqGo3iq6++wsuXL1Gv15fq9y8NwzCwsbHBNhAefk3aMAzs7u7CdV1cX19PtSVJEtbW1vD73/8eo9GIjXkQNE2DpmkIhUIwDAPHx8eBGokkSYhEIsjlcshkMp72aMOeB1mWsbGxgc3NTYRCISYbJKd0D1mW4bouW3uKoqDT6SAajWJ9fR2j0YhpwaTVRyIR2LbN5Ifkg+5LsuG6LvtdIpHAv/3bv+E//uM/2Dgkk0noug7HcRgpjsdjz/hRf4m46Lp5+CBSCofDODw8RDwe9wzULCiKAtu2kc1m4bouXr16FfibYrGIQqHAmNx/DPRD0zT2u2g0irW1NVxcXLDvu90uExK6B+0EoVAIw+GQ9Z2IkgRgd3cX5+fnUwSq6zo2NzfZc/v7s8x5Px6P4/HjxxgOh5AkCZZlMZLsdrtMM+SJiewAQX2SZRnZbBbr6+uIRqOevtHz9vt9xOPxKZuXoijY29v7LEiJdvuNjY0pQiA7Er/ggLv52tvbw7t37zwbRigUwubmJjKZDJuHoDn1Q1EUZDIZ3N7eBh59VVXFl19+ybRUXn5pTuchEol4NDbgvUzSMa7T6cAwDKiqytqnjVaSJLx8+ZL1LZvNYnNzk21U1A5dR8RxcnLCNnFe5gDgL3/5C+vL2toaUzyorXK5PPO5aLNVVTVwI/eM3dxv50CWZayvr3tYj1ffEokEXNdFu932TAhN+Pr6OlzXxevXrz3txmIxFItFj6ZFD2RZFobDIeLxOFv4frJyHAe7u7uo1WqMbEqlEjY2NhAKhdBut3FycgLg/U6Ty+WQzWbZjkuGO5q0f/7nf8bf//53dg9FUfDll18ygfHbf+YRKCEUCmF/fx/hcJhdJ8syrq+vUalU2P/JHpbL5Txj+OTJE/z000/sM0mSkMvlcHBwwPrlui7G4zGazSZKpRLbEVVVRbFYxPr6uqfNcDi88Mj6S0OSJHbsJo3DcRxYloWrqyv84Q9/wF//+lckk0lks1nPZqnrOp48eYLnz58DuCM32lQJ/MIfDodIJBJMi/XPtyRJODg4QKPR8NhJVFXFwcEBszP65WMRFEXB0dER6zu1TZvpmzdv0Gg0YNs21tfX2eao6zrTbiaTCbtOkiRmWyS7G5k9hsMhnj17xjb+wWCAi4sLbG1t4eDgAIPBgBm+aW3Lsoxisci0NOCO9G9ubpbSAhcdWx9MSvF4HJubm+z/vV4PrVYLjuNgMBigWq3i8PAQ/X4f2WzWY1CjiY/FYgiFQhgMBgDuNIzDw8MpBiYP1IsXL9Dr9bC5uQlFUWCaJnK5HNsZ2EOpKo6OjvD06VMAd0a28/NznJ+fBz4LGfcODw+Z1gWA7Sg8ZFnG3t6ex2jJH42W0ZAMw8DR0RFbDHSPd+/eTZE0jS1/f+DOVrG+vs6OI7FYDHt7e+woSmg0GlNGylAohN3dXQyHQ8+xgLx/qwieCHjtwbIsnJ+fo1qtot/v4+zsDABwc3MDx3GwsbHBfuu6LtLpNPL5PJrNJra2tjyEBNzN3+3tLZrNJrrdLkzThGVZiEQi2NzcZNo2bVySJOHw8BAvXrwAcDe3jx8/Zs6eoE1znh1K0zQcHR15NBo6UlWrVdTrdY9trFwuo1KpIJFI4Le//S2GwyGGwyF+/PFHJosbGxtsU6N5tm0b1WoV5XKZrT/CaDRia0XTNDSbTc89yWY1HA7R7/fR7/eRyWRmPtN98WBSisVinrNns9nE27dvIcsyG4yffvoJqqpiMBhgZ2fHQ0yTyQTRaBTJZBKDwQCqqqJQKDBthSaPdnsiJADsaGaaJtN0eFvWZDJBoVBgpLQMarUa9vb2PK5XerbT01P2u3A4jEQi4enjeDxmGs8iw6emadje3mZt0DO+e/cOb968CbyGhJLgOA5M00Q6ncb19TU0TcP6+rpHu3QcB/l83qNNEfgjK9+H4XAYaHP5pUFaYK/XQ6/XY9qGbdu4vr7G+fl5oAZCckjzaFkWwuEwUqkUFEXxaJ90fbfbRalUYsdbkrlOpwPXdXF0dMTaG4/HkCQJhUIBL168gCRJ2Nzc9GxYfswzBlNb6XSa9Z/X0EqlkmeD4ttsNpv47rvvmKZEzx6Px7G1tcW0LLrPcDgMNJ8QhsMh3rx549kACOPxGMfHxxgMBhgMBphMJqhWqx8lvAZ4ICnFYjEUCgXPQjFN0+N1I0wmE9ze3iIajTJbEqnGiqKwyXccB+12Gy9evIBlWej3+x5WD3rg4XCISqUCXdehaZrHkHvfmB5JkhCNRj02JfLGkZ3FMAw8evTIs1vydqhlEI/HkU6n2djREZTU8Vn9ItBRlmJLgLvwBV4jIA/l8+fPp0IIQqEQOzrzHhOai4fGQn1KJBIJZLNZjEYjz/GEZGbWkSiIzC3Lgq7r2NraAgDPGMiyjH6/HxhjRuPOg47qrVaLmTPoKEVHZeorb4yfRUqqqiKVSnlsWrTJn5ycBBISj1ar5fl/MpnE48ePmV2Ljm10ZFsGQTI5Ho9xc3PDvH2JRALD4fCjxYM9iJT8xxpysYdCocAzpW3bqFQqqFarbJL8gYuO46DRaExdu4h9e70enj59yjxn9Oe+L0QIhUJoNBrMwEfuXxIy27aRy+U8MVgkyMuco+kexWKR2RrIvtNoNPDu3buZ1/kFg56NiCWbzTINEbhTv0mDLJfLjGgURUGhUGDeINpR6Rgyrw+/FOiITlohALZhdbtdVKvVwOskSfIcRXnTQTgcnortod/M0laBu/ELcuiQd4p3fPDy0ev1ZoZ78Ein00gkEh5C0zQN3W73XjFvwB2ZRSIRj32R5Prk5GRpmQ2CYRhIpVKIxWIwDAPpdBqvX7/+aFr2g0iJDLAEMiLy9hg/RqMRRqORx5pPkzyZTBCJRNhDktdJ1/WFAYO8t4QW2UPe0NLr9dhxk55RURSUy2VMJhM2EfzxU9M0OI6z9A6xtrY2pdo7jsMM77PAPw+/IVBf/NG9NKaapuHg4IAZMguFAjY2NhjJEikCwOnpKcrl8lLP8XMiEokwEs1msywQcjAYLBw3Mvz6tSVefukYpus63r59O3cug44ydB8KXyFYlsXse8vIhyRJzPHAEwiR787ODobDIVqtlid8ZBZUVWWBuHx/arUaHMdBoVBgGzBpyO12e+HaIa9gIpGAoihMa+fX4aK+LcKDSYkHxR9sb2+jXq8HToKu68jn80gkEizWw3VdhEIhvHr1Cvv7+54BJMJaxL75fB4HBwdMvab0kdFoxHY9wzCYgNJEk6bGkyQdK3ljJBHiZDLB1dUVhsMhQqEQi8949eqV5+g0CxRcygfl0dFtkVpOml+QV891Xdzc3CCdTjPNgI/DisVi+Jd/+Rf0+31EIhEWQ+W6Lls0p6enK+lxUxQFkUiEaXJra2solUoYDAa4vLxcOG7+cAEaF7+bnf6/qD0i8iD55zcsGltZlvH8+fOljMCZTMZj3Ka4n8lkgnw+z+bKdV30+33UajUW7Er3bTabbHOiKGve5Q/caetHR0cA7swf5KDq9XpLEQkFZ/KBqsPhEJlMBru7u7BtG71eD7VaDbVa7V5R9Owe977i/we/QMj2sra2BkVRAkmJ4kFIjXVdF4PBgO2AfEwStd/pdBZGGdPAUF4bkYXjOIyUCoUCcrkcE0DHcTAej5mhkjdqW5aFwWAwFQdEXpnb21sWsEfxRJFIBNFodK6Krus6i+qlfsuyjPPz84XCEHTM4DEcDtHtdj2kRKRPdg9yTPDR8JIkoVQqrSQhAWBR97SgKEr9/Px8qaOmPz6ID4kAwOabPExko5vXn6D2aAOl/9Nn7XYb3W53KVLye5BJs+FTXehvyl/k14okSbi8vGTe22w2OxWVThpRr9cLzNE0TZNlDmiahlqtNhW3xkfO071N0/SE04RCIRZJ//r163sT04NIyb+IhsMhwuEwnj9/PrMDtm3DsqypieXb9O9kZN2fhyAXq799XdenUi34XDVemOr1Omq1Glu8qqpie3sbuq6zXDm/Qfjy8hLv3r1jxBOPxz32DF3XPVoSvxgeEqzofz5y4RqGwYzifpXaP2adTod5UFYZ/meNxWLM+bEIJBu8a52OtXz7kiSh2+0uTKImTYXXPnjNie6nKAosy8KzZ8+WSkCl6HReg+M3b7rnZDLxpIbw/aLwh1QqhdPTU5imiclkAtu20Wq1UK1W0W632WfAHRGSNzKTyXi0UuAuE2I0GuEf//gHgPepJXwfguy3ZM5ZW1uDaZrodDp48+bN0maOB5OS39ANYO6kkrHu8PDQo+bSQ/Bt8drLMn3h2wu6ptfroV6vI5FIQNd1j6uYBpDUWAqTJ+RyORQKBSiKgnw+D9u2MR6P8f333zNjIWlf3W4X3W53KjeN1PF+vw/TNNnvl3lGitjlwZMbgXKx/Dl0/rEmR4BhGCzy/UOyzX8O8GMUiUSQSCSWInO/jAZpnEEBsLPgX1QUsEgg7Qa4iw+jDXpZm5JfC+OPi+TpouMRHfX4vhEhRqNRtNttXF5eotVqTWlDlDybTCYRiUQwGo2wsbHhOZXQeIVCIaTTaWaWGY1GLLSHyNiyLJYYzI8zXR8Oh9FqtZbK2QQeSEp8Dhk9wDLghZ+/hs6+5O51HAeGYSzlsQjSlPhIUwC4vr7Gu3fv8Jvf/Iadzel+tAOpqsrU7EQigWfPnsFxHITDYaaaj8djaJo2FcFLO9RgMECz2USn02GBicDdpJVKJVxfX8MwDITDYUQiEei6vnB3jsfjHgGcJeCkevNaQBDp03OHQiEUCgWEw+HADPBfGpIkYWtry6NxUmJoNBpdSEp+7xvZ7+hYC7wfy2WDRmeRNx9WIUkSrq6uPF48fs5mrRX/535HxLt373B7e8vMHmtra/jqq6/Q7/fZhlir1dDpdNDv9z3eNV3XkUgkGBmZpom3b9+y2KvxeIzr62tEo1Hk83k2fsD7HM96vQ7XdRnJ9Xo9xONxFvLjOA50XUc2m0U6nWZrhkJNNjc3UavVloplejAp8QNNgYPRaBTVanXmwPt3cFogswK0fvjhh4V98d+LVNwff/zR8xkA/PDDD5AkCX/4wx/QbDaRTCYRDoeZsNHf0WgUv/3tb3F+fo58Ps/6SaknFA+i6zrLn6Mw/nQ6zQIRqf9k8xqNRqyUBj3/ol3Ub1jlg/zK5TI0TWMJ0fyRlGK9yJPJZ3LT35qmIZPJ4PDwEMfHxwvH+ueG3/5DJWaWyU1Lp9MeMqcjHxEH7xXrdrssInsWJMkbL0YgQuIN03yAJ+ANa+G9pwTKTvC3qWkaer0efvjhB08+HwBUKhXU63WEw2F0Oh1PIqyqqohGoywmLpFIMDtXpVIJzJvsdrsoFAooFousvzRGFAV/c3PDIub9JxQCnQb4sVIUxZMfuwgPJiW+I3SuPDo6wvn5+Uy7Ermj+ev8i44nq0XHCkVRkE6nPaQ2T6Ogz/77v/8bwF1wGSVjEqvzmtrW1hbbWWmnrVQqkGUZBwcHrPwDHwWuadrU80ciERwcHMCyLPR6PXS7XRaivwgkGDRGFIZAQru1teWJ5qbd7M2bN+j1eiwmJ5vNwjAMJJNJT0mKyWSCRCKBSCSy0Pv0c4KInceyeYXAtFbDH5kJ/KJatDnQ2AV53uhv8v5Go1EUCgW0Wi2WOxZ0DSEcDiMcDrM+kObe6XRYMHEQKMIdACuwRpo15YcSYaqqivPzc0YqQSBNhgiHNmF/CAUhSPnodrs4Pz/H48eP2VqmfhSLxYVhHMADSYmiafldl4Ro3lGu1+t5Yjn4Mgk8gnaTIPhVdFpsVEZl0aKno9YXX3zhISYSUjojk+ZBdiWqDRW0QMjOoCgKvv76a1iWxQqOSdKdW5vG6eLiwlPNIAitVgtPnz5FLBZjBbV0Xcfu7i7q9ToKhYLH0GpZFi4uLpiwUm5Sp9OBqqp49OgRU9EJ5D1cJVICpoV+2Z0WmLa7BRHasgRH7fAy72+HwlxM00QikUAul8N4PJ4iVrpW13X87ne/A/B+o6H7UN+bzeZCRwSVZSG58BcIJCwTXmNZFiqVCorFoqev94056na7nmsoLi6bzX46UiJ3OhBst5gHflLJvkPBa/QAy+xcwLTGRtf87W9/W7pCo23bKJfLWFtb83xOmgT1ldRrstn469HwcRv0J51Os3AJfnyozG2QJ4UHLULy+l1cXLDrLMtCLpdDq9ViGhulxARFOVNgabfbZUF6tBuuKugYxNsnlkWxWJxaVLIso9VqsSRcv+1xHugYTCAZ9VeqIC8fHev9dlFZlmFZFjOKk0udtPHhcIjBYIB6vb50hH0ul/PImH9N3mfc/L+ljAU/uc7D+vr6lMlBkqSlk3YfLJHn5+eIxWJLVZ0jkHuUn0RN0/D27Vvs7e1NLd5FoCp5BJrU+7rZ+WMXEQppg/5Jopw93h7kD2Wg8z3VweYFl8hulqeQoGkaS0khwyIZMUlArq6ucHt7ywqOhUIhdDqduc9K9kA+9ue+O+HPBTLeUv2dZfuaSCQ8GjQRxcXFBW5vb/FP//RPgR7MRW1SWAkRElXF8Jc+Ad57pCgEhuZblmUYhuG5L206FDs0GAyWdp/ztjK6v98DRieKRZqmqqosGZjasm0b7XY7MAVsFvjTEP+c846Onn4sfScfWq0Wi7XhmfmLL74ITPYjFY5fiJPJBKenp6hWq6zCHg9d1+fm6FBKCvB+5+LrCy8Ly7I8wYekvZHdZjweo16vs3ox/iNnOBzG48ePPcFqk8kEL168QCaTwWAwQKFQ8AS80TmfYl38iMVi2NnZYb+3bRuj0Yhlb1MfyKDtV5lnYdmj8SrAsiy0Wi1PjBfFGc17jnA4PFUxwXVdlslOR3H6jjTGWZ4hcgjQnJMck7MhSBPhjdUbGxue9B7btpk21Gq10O/3PRUBqLIllSGZh1wu57EnzooHpGTmWZAkyVMlgz/23ufYvLa2xipg8H1SVTWwLE8QHkxKruuiVquxHDXSLmblv8ViMVYKl64nVXYymaBWq3nsI4ZhYH9/f6ZXRFEUFnBGMR0APOUYeFvLPEjSXf4Pn6RJdb+fPn26MHOewgX4sivAHXGTp85xHBwcHDABomC3m5ubqexuTdOwv7/PCIsWFRWpGwwGePLkCcuc73Q6TIuapyUmk0lsb2977GaU9LmqoPK0fCJsoVDAzc1NYLS/oigswJLkQlEUXF1dodlsQtM0tFotT/lY0zSxs7ODt2/fBvYhGo16PJvAHTmSfWSem9uyLJydnTFjda/XC3yJAeWqpdNppnnd3t7i9PR0pvyScduvcZPXlTeFzDvJqKrKIrn9jgAi32UgyzKreMqvc0mS7vXGnAe/9811XZRKJc8icF0XpmlOGVKBO2Mqr4nYto1Op4N6vQ7btnFzc+MpS0vC9+jRo8D7b21tseRWGvCzszM28MViEV999RUeP36MYrGI3d1dPHnyxGPzAd5H0/JnfxrUSqWy1ITQ7jQPZB+h+5KG+ec//9mzE2mahkePHrHKmrzaTwXNqD2qFpnL5bC9vY3Hjx97Cu/xIIMoGe8BsONps9m891tSfi60Wi00m03PZuE4Dv785z8HHn+peNloNGKeVMuyWJY9OQL4o7qu63j8+LGnEiePw8NDT4Q1AE+NrUWYTCYol8sol8sz88F0XcfOzg7zklL2/e7uric9i/ocDodZLql/vE5OTqaSaykJl5d9av/g4IC9nICHZVk4PT1d+lmPjo6YYkFePwqVuLq6WjpI94OsnI7j4Pz8nGX4kwvx8ePHzKhIZ1nKYaKOybKMRqPBgge73S5ev36NL7/8kkVdk10mmUwyjxVN1sHBAXNfOo6Ds7MzT2XJjY0NFj/F1wR2HIftLtS+32NBGsqyhkayZRFUVZ06dlqWNWXQpx0kFothPB7jiy++YKH/fH9l+a5+8rfffsuOphTASZoXkdze3h4k6S5v0DRNNBoNRCIRZLNZT4UC0hIoHWLZXeznBhExBeSREbndbrOSy/1+H+l0GoPBgGmL3377LUtyLZfLniDVdruNm5sbbG1tMQ2fDM+JRAKDwYAFw2azWVYSFwCrbXTfMh26riMejzM7rGEYHjMHvXetUCiwuQiHw+ylAZTykc/n0Wq1PEG9wPvYtOvra/R6PVQqFVZmh8Ytn88zY3O73UYymWT2Lb+9znVd9tYWQiwW89hDQ6EQ6vU6OwXxxRb5Y1u5XL7XK7ykBefMpYwPyWQST548YV4EPgGQoj5JjabvLy4uAt2Dm5ub2NnZmXrnFtX6DofDLJCRvrcsC//7v//rIYbf//73gaotwf/cvI0iGo3iL3/5y1JxRISdnR3s7u6ydjqdDr799lvPbzKZjKe2N/A+noYWQL/f9xhTKYCw0+mwXCrgTmN49OjRVLE9Okqoqop4PI5SqeQpKkfCous628FLpVLQ+CzvcXgglpUvWb4rQUx2GfII0b9rtRr29/dRr9dRqVQ8GvO8No+OjpDJZNiCJrmt1WrY3t5GtVplGhUddentIPdNMi0Wi8yZ47ouer0eyynj+/Sb3/zGY2z2yy//XH67WqPRYNVWqawu/57ERZ5y+p769/r1a+Y4UVUVX331FQtt0TQNiUQCL1++ZAHGfBvkda5UKoGa1jz5+ij+4GazidPTU1Z+hAaeN0LTZ1TwbVZm+tXVFStszxMPpaLwOWO0cG9vb6cMgu1220Ng/snk/+bP2+PxGM+ePbuXCxS488rxmkuQgb7RaODm5obVFQfea0zRaJTFpJChnXKdnj59yt4uSiCvSC6X8xA4/6ztdnvKm8Ibdnu93krWUPLDcRyUSiXI8l11Rzp2kpaTy+XYpkUOk1qtNlfTdRwHx8fH7PlJBugP/740/3cPAZEen7IU1KcXL15gZ2cH+XyeVR/wy67f2+26dzWX+E2enDeFQoHVMePvw4Nsb+Q4GY1GePXqlWdTTqfTLG+T0Gg02CmE9zrSMy4K1pyFjxakQuUxyR7CezT4xU9vjp11XKCjmKIoLOSdnxx+UgaDAW5ubgIJrlQqMe8WxQuR5sGXzuULnlF7s2pCzUOtVkMul2MqbtD52XEcnJ6eYjgcsiJb/He0u1C/arUaSqXSTI2NXhf0+PFjdnzzewZJeG3bRigUgm3brMTFqh7ZgjCZTFAqldjxncaJf10Rby9cxkZm2zYbW0pUJu2LFhYd5e/jpg8C1bKmTWuWfWUymbBa95FIhL2jjYzVZKAnYqMQAjq28Tg7O0On00Emk2GvgydbJa1LOg4rioJ6vY7b29vAxFn+1U3AdCE6IlzKWuj3+4Ea+DL4qJFzt7e3UBQFBwcHAKarCQwGA1QqlaVU39PTU/R6PVZTOZlMIpVKsXpJvV4P1Wp1ZtH4yWTCEk159ZLAu2ATiQTTth5ayoNew6xpGrNRBIEMrf1+H7u7uzAMA81mk9nX+Gqcs0q98qjVaiiXy1PGeuDu+NxoNNBoNJj9DMDCKPJVxXg8xps3b7C/v+8JxONlrNlsznxB5CxQnSbSBnhict27F1jeV3P2gwzv1NdFbvbj42OEw+Ep4zNtMERKw+FwblJ3vV5njoJoNMqyDfyQpLtyyLPIcjAYMBsR9cMvX4qiYDAYfPCblj+KTcl3Dcu9CYVCrGQmGfIeuuiJ0SnVgrx1H2O3X7Y+z8cGaXL82yceAtJKnzx5gsFggNFoxPKuaJe9L1bJpuQHvYmEalCTxkMZ7A95qwZtBlRAjciJjo4PqaDoRyqVYuVzxuPxgzWJXwqPHj2CqqqsLO+nkq+PTkoCvw6sMikJfP6YJ18PjlMSEBAQ+BQQpCQgILBSEKQkICCwUhCkJCAgsFIQpCQgILBSEKQkICCwUhCkJCAgsFIQpCQgILBSEKQkICCwUpgb0S0gICDwc0NoSgICAisFQUoCAgIrBUFKAgICKwVBSgICAisFQUoCAgIrBUFKAgICKwVBSgICAisFQUoCAgIrBUFKAgICKwVBSgICAisFQUoCAgIrBUFKAgICKwVBSgICAisFQUoCAgIrBUFKAgICKwVBSgICAisFQUoCAgIrBUFKAgICKwVBSgICAisFQUoCAgIrBUFKAgICKwVBSgICAisFQUoCAgIrBUFKAgICKwVBSgICAisFQUoCAgIrBUFKAgICKwVBSgICAisFQUoCAgIrBUFKAgICKwVBSgICAisFQUoCAgIrBUFKAgICKwVBSgICAisFQUoCAgIrBUFKAgICKwVBSgICAisFQUoCAgIrBUFKAgICKwVBSgICAisFQUoCAgIrBUFKAgICKwVBSgICAisFdd6XkiS5P1dHBFYLrutKn/oeQr7+72KefAlNSUBAYKUgSElAQGClIEhJQEBgpSBISUBAYKUw19D9KSHLMlRVxWQygeM4P/v9FUWBJEmYTCb3uk6WZWiahng8jkgkglAoBFmWYds2BoMBOp0O+v0+LMuCbdtLtytJElRVhaZpCIfDSCQS0DQNl5eX6Ha7v8gY/RqhKApSqRQ0TUOr1YJlWbAs615tyLIMXddhmiZisRjC4TBUVYXruhgMBuj1euh2uxiPx/eWL0VRoOs6bNvGeDxmn2uahmw2i+Fw+EHypaoqIpEI4vE4DMOALMuYTCbo9/toNpsYj8ewLOsXlbefhZQURcHm5iZc12UCEI1GEYlE0O/3Ua/X0W637yUc6+vr0DQNknRnxHddlw1kq9VCt9ude30mk8Ha2ho6nQ7G4zFUVYWiKIG/dRwHlmUx0ojH40ilUuzePFzXxXA4RLPZZGRi2zbevXs3sy+JRAKJRALxeBzxeByaprG2YrEYrq6ucH19vbSAb2xsQNM0NJtNtNvtpa75XBCLxZBKpQDcjQ8/B7qu4+TkZOaCMgwDxWIR6+vrbEPsdDpoNpuwLAuKokBVVbTbbdTr9cA2SJbz+TzC4XCgDADAaDRCq9VCq9XCZDLBzc1NYDvAnXxNJhOoqopYLIZEIoFms4nr62s0m03W9+3tbRiG4ZGvyWQCXdcD+9Fut9FsNiHLMnK5HLLZLGKxGHRdD+yz67rodrtot9vodrtwXdezxgi3t7cYDAaBbXwMfHJS+tOf/gTbttmilyQJkiRBlu9OjolEAplMBmdnZ7i6uprblqIoSCQS2NvbYwRCAsiz/mg0QrPZxMnJCVx32ussSRKi0Siy2SzW1tZYG7T7yLIM13Xhui5kWYZhGOzfw+GQPUNQ2wAQCoVgmibbOUejUSApSZKEQqGAzc1N1n/6nP4Oh8PY3Nxk5D1rXFzXxTfffAPHcZj2BuBXQUqyLCMejyOTySCTyUDTNKZFyLIMRVGgaRoURYGiKHj9+vXU3IRCIWxvbyObzTLZUVUVqVQKqVQKjuNAlmU2xz/88AP6/f5UX3RdRzabRSQSYTISBF3XkcvlEIlEPKQkSRLW19eRyWRYG6ZpsvvT3EciEWQyGQ8p0fe8fDmOA0mSMBqNWF8kSWIE22w2YRgGCoUCYrEYAMzsM3BH+tFolK0J0thI0zIMA/1+//MkJcMwsLGxAVVVYZqm5zs/8xqGgd3dXbiui+vr66m2JEnC2toafv/732M0GrGJCIKmadA0DaFQCIZh4Pj4eEoDKxQKWF9fBwAmiK7rztSUqA+qqsK2bTaplmVhPB6z6+n+RBKqqjJy8CMSiWBnZwe5XI5d6zgOuwf/fKZp4ujoCCcnJ+j1etA0jQmNruv4+uuvmcpNWtZoNJr5LJ8TaEHl83mPRqCqXtGVJAnj8RjZbBaSJOH4+Jgdb1RVxd7eHjKZjEezVhSFHb1pnkhj+d3vfofvv/8evV7Pcx9VVZmmYds2G2fapEi7IFmidqPRKFRVxfr6OpLJJAzDmGqX+gVgSkPRdZ21STJI8uW6LkKhkKe94XDI/h2JRKDrOiaTCdu4LcvybK6SJLHnIqWB/iaZkiQJpmnO1LQ+Fj4JKUWjUezu7mJjY2OKEEht5lkduBv0vb09vHv3zqN+h0IhbG5uIpPJMC1l1kLnoSgKMpkMbm9vcXt76/nOMAzPEWk4HHom3Q/bttFut1Gr1RAKhRihkfZXKpU8fabni0QisCzLIyB0/ydPniCZTDICol2v3W4jl8uh0+kwwSCB+eKLLzAYDJjNAbgTXlpIBFoYnzvC4TAODw8Rj8entMggKIoC27aRzWbhui5evXoFANjZ2WGEJEkSHMdh8ifLMps7Wpw0nl9++SVevHjhMQWQBkLXhkIhlEqlKfuObduwbZv9/osvvoCmaXM3KpJF4G498BoNT1KO42A0GqFWqyGZTCIWi3nadF0XFxcXaLfbkCQJiUQC5+fnGI1GTJucBSKrVCqFfD7vGW9JkvDmzRtcXl7OvP5j4KOTkizLTD0lFZtsMldXV/jDH/6Av/71r0gmk8hmsx5h03UdT548wfPnzwHckRsJJYF2I8dxMBwOkUgkMB6PPZoOf/w5ODhAo9Fg9hjSeOjfjuNA13VcXV2h1WohHo+znWA8HjMDIO0u2WwWhUKB9UOSJFxcXATaMYKOeJFIBPv7+4hGo4y8BoMBLi8vUa/XMZlMYBgGJEnC3t4eksmkx3YSCoU8zxAEWgyfM0iOksmk53Ma50QiAdd12cLjrwPubI60OPP5PNMoJpMJ29iurq5wdnYGRVFQLBbZIlRVFY7jIBwO45tvvsHf//53trnymrXjOFAUBZeXlzPtfaSlhcNh9hl/7IvH4/jHP/6BbDaLs7MzpnmRRsO3Q/JKZpBSqcQ0N3K8kBZ9dXXFiPLs7GxpeyQ5oPL5/FR/e70ezs7OlmrnQ/BJSInfpS3Lwvn5OarVKvr9Pnuom5sbOI6DjY0N9lvXdZFOp5HP59FsNrG1teUhJOBuwd3e3jJDn2masCwLkUgEm5ubiEQiAN6TjyRJODw8xIsXLwAAW1tbHlIhW9FkMgnUqvwgUiDj5DyNxE9IsVgMBwcHbEGRYF9fX+Pi4sIzZoqiTGlYRE58u7z6TZhMJp89KcXjcWYIBu7IiMa8UqmgWq3i8PAQ/X4f2WyWzQvwfhGFw2Hs7u56jh9ESDc3N3j58iVr//LyknnS6HfAnYaSTqeZTcg///PGWVVVPHnyBJlMht2fiPHi4gLD4RD1eh3j8TjQbEEgj5zfW0yOI8uypo6ZPJYlJEmSsL29jZ2dHU9/bdtGs9nEy5cv7+1NfAg+CilJkoR0Oo1+v4/xeOwxkl1fX+P8/DxQkyAmp4e3LAvhcBipVAqKoiCXy7Hf0vXdbhelUontKDQZnU4Hruvi6OiItUcGukKhgBcvXkCSJEQiEaiqCsuy2ACPx+O53jH+Oflj333cpqZpYm9vz0OyjuOgXC6jUqnMvB8/PrTLk/1kNBqh3W7DNE2kUimm4f0awgdisZiHHIC7cbAsix0ffvrpJ6iqisFggJ2dHQ8xTSYTZlck8Laj8/Nzz/263S4uLy+xv7/PtCE6xm1tbTFS4jeheQZjMmjTfPNzeHV1hXK5vPQ8kTF/0T0/FKQQ0DGY1qTruqhWqz8LIQEfiZQSiQQKhQLOz88xHA7ZgicbyazB570NwPtjnq7r2NraAuA1IMqyjH6/H2jEpQnnQQbxVqvFPuONfATLsgI9LUHgd8ZlBUTTNGxtbXm8H5PJBLZtz/Soqao65XJ2HAeGYeDly5eoVqtsXEzT9BggP6Xg/hyIxWIoFAoezYc0HL89ZjKZoFqtIp1OwzRNj93Fr8WS+zzIpuM4DpMB3t5EcqfrOsbj8dLzb5omksmkZxMjp8hgMLjXxkHetE8FSZKwsbHBtEpSFsgkcn19PRXS8CnxwRHdpmkil8t5FtxoNIIkSeh2u6hWq4HXSZLk2cV4AQyHw8yu4r/mzZs3M/tC3hS+PWqT/w3fnp8Y58EvHI7jMI9POp2eKTjpdJppf0Sysizj9vZ2ZjwVqc38vQlkROUNtss+w+cA/xGV33C+//77qd8TofDjRccdXtui+Xn79m3guDebTY+rm7Rhcrb455/c5alUCtls1qMF5/N5JBIJ9jx0f8uyFsbQBY3HMs6dhyKVSqFQKASeAm5ubnB8fPyzbnQfrClRPAUFFgLvJ/fk5GTuteRh8GsD/I5IO4yu63j79u3cHWbRbqIoikdwHjLRvKuXQh0ODg7gui5ub29RqVQ8MSOyLGNtbY25bGnCx+Oxx440C/z4zBKMIA3ic4b/eRYtSsdx0Gg0kM/n5zoAliHubrfLjNJ8qIc/FIHihQDg8PAQkiSh1+uhUqmgXq9P2Vbp6Njv99HpdAAEy2uQWYB/flVVMRqNmCeNx30ivAnJZBK7u7vsmXmv4fX19cI1/CnwQaSkKAqz0ciyjEePHqHVaqHX66FcLs81vgGYChegXc2vItP/F7VH2sMsAd7c3PR4Qah9sonxi7/ZbAYSIK+J+RcAxdNUKhW0Wi00m00UCgWsra2xaygEwLbtKUO2H0Ga4qzf/Zo0Jf/80aJcZMMhW1u1WvU4Ouh7AMzbGQTSyEiTBbx2KL5vQfOfTCYRjUZRqVRY2ol/XiiMIBKJ4PDwEMD7MBlFUdBqtXB5eYlGo8Hkj59fihUyTRP/+q//ytqkU8Qy2Qx0TTabxfb2NjMT0HMTeR4fHy9s51Pgg0jJMAyPW980Tayvr+Ps7GyhFwuYXkzk1eADySzLYikTi+w+/iOfXyBoF6L+UuCcYRj4zW9+wz4D7tTWWq0W+Bx8OAB/HyLUzc1NFAoFVCoVtpv6jySLtKQgovk1Ec8i+I+rFFk/awwcx0Gr1UK1Wl1K9maBcg0Nw2BHwHA47MlDIwQdnVVVxfb29pTdkw8jODo68myOPIFSytHFxQVOT08B3NnCarUaJpMJEomER2vivcgHBwcYDAZ48eIF08ZmQdM0bGxsMPkk7yaR0qeORZqHDz6++YUkkUggEoks1GoAMJsIz9J+Nzstzm63uzBK2U8W/h230WiwhEzy7lAwGV1rWRZkWUY+n8fa2hoePXqEp0+fYjgcMi/E5eUltre3WfAkaXNEppqmYTQaMdeq/5kVRUGtVls4Pn672Lygu18T/M8zHA4RDofx/PnzmfmRnU4Hx8fHTOPgtSS+TTIHzEKv10Ov1/PYoyjA0nVdvH37FpVKBeFwGOFwmGUQmKbJZInspTxpkYwnk0lPUCx/zKffuq6LQqEAWZZxenoKy7JQLpeh6zq++eYbFktE9wLeR3mHQiF8/fXX+P7772du4pIkIZPJeLyVREiO4+D169dzQxQ+NT5aSACBbEzLkJLfmBmkGfgDIueBP24FtdfpdNDpdNDtdpHL5VAsFj1HKLID0K6hKApCoRC++OILfPfdd3Bdl+0gZMAvFAoIh8OwbRtra2tMFTYMA+PxeCppmPIAFyESicw8xvrBB+P9GuB/Hvr3vE2Jjl6JRIJlAPAkTp6vXq8XaCwn0KZFMUHj8ZiZBQBgMBhgMBh4PLrA++Bf0rDo30RalKFPZBIKhdjmxW/OvMwWi0U0m03UajUWLPw///M/LGwmk8kwzYmM6HSy+Oabb/Djjz8GakypVApbW1uedBkiynfv3v2ihAR8AClJkoStrS2PB40YnNTsRTYAfwwJxQHRdSQIiqJM5QoFwW/oowny9+P29hb1ep3FXuTzeWQyGbbzkPZDIQXhcBjZbDbwWMCruZZlYXd3l5GOP0eI2uZ3qFlIp9OefCZ/Kokf/kX8OZMUb0sBltcEI5EIOxr5x3eZjQC403KLxSKi0Shs20a324Wqqri5uZl7pCGvcxBxkgZFhERHUYqXA+5MH/QZr3EFbUT9fh/9fh+tVgv7+/tIJBJs7VBSOSUO+0nJNE1sbm4yJw3lw0nSXVLvMs6XT40P0pT89h8qAUIBV/OQTqcRjUY97lKy+hOh8QGTFJE9C5J0l/kfhJ9++mnqM9u2WexFo9FAsVhEsVhkmg31i46TfgN5EK6urhCNRrG+vh5IOiRkT58+nXuEAN7nIPFHUUmScH19PXX0I2/ecDhk5Tjq9ToajcbCPq8iSFMl0NhHo1EWnxUEIvKgsVcUhcUumaY508lAc0RajW3bME1zSjOahUQigVQqxYJbeU2LKljM8pL5bZTAfELudrt49uwZkskkczjRejQMA5FIBOFw2HOMoxpgFFhMGtpgMMCzZ88+afb/sngwKbmuOzWx87L3/fBPDAlNkKF6mehpwzDYeR2Ax/uxyFVKkdWtVgtPnjxhGoqiKCw+Zm1tDdfX13OPEFToLUhL5EMblomM9cdTkcYXpAWNx2OUy2U2J4sIb9XBx18B70uzHB0d4fz8fKZdieww89qNxWIoFosz493C4bAnb/O+zgXTNLG9vc36QQHBlmVhNBqxiPSgkjJko6IaXERm8+bTcRzU6/WZQbh+tFotvHjxgmlspAR0Op2VICTgAzUl/+K4T6yMP2w+iNDuIxBEXERElJJRKBSYy3UR2u02qtUqisXiVFwKJeouMra/e/cOmUxmKmfvPiCvJj0TVQbsdruB6jV5nkiz0jSNVShYxra3aqBsAN4JQoQ7T3Po9XpTJTz88Fd09INIhHCf4FoATEOlfDc6qlGlSqpHFERKW1tbSKVSbN7J6fLdd98t3JRDoRCzbU4mE5Yx4H8Wx3HQ6/U8hv9VwweRUq/X8xSnuk8oPL/weQ8EZerT/++TyuEv3yHLMv7f//t/C+OB+GsuLy+RSCSYd46INhaLLVUOhNdmHmrXAO6OgqPRCL1eD/1+f6FQZjIZVlRM13VomobT09PPkpTG4zEjjvtGrPvHnb9+mU3Tr4netwQMGbj9/aD1cXNzg3K5HHjt2dkZq1Tguu7cEAg/isUiCoUC+z+ZQr7//nu0Wi1EIhFsb2+zsjdUf2s4HM7Mvfyl8EFhwN1u12PfWJZE+PIgwHtBuri4YOVM7xsQmEgkWO4TCVar1bp35UXyrPDCrKoqbm9vlyK3SCQyZdviSXeZ8RmNRiiXy7i5uWEldWeVPCUkk0nkcjlEo1FP/tLnCv6YtqzBm7dlkgyQq5wPqZjXDmmmfjJcdsPVdX2mtmbbNuLxONLpdOD3u7u77N/+kIFFuLm5YbYjIkB+I8tms8hms0gmk2wDo1PEquGDemRZFlqtFvNcAe8Nw/MGlNyqlBxJqjp5w6h0B/D+WDfvCKZpmqeqIHnyWq3Wvc/J5M4lUATw5eXlUkm79DxkyyL7wHA4RLfbZXWb511Pqn40GkU0GkU4HIYsy3MN5ORapnG7bzH8VQMV9ee9p/MWKAXC8sRNssR7gx3HwebmJmq1WuB8Ujs8CfX7fRbIOA+SdJcZEKQl0/fzSGsZ0pwFv4GfCJm+o4wFClCWZRkXFxcolUr3vtenxgfTJC0GPvirUCjg5uZmppZiWRaeP3+O9fV1VjC/Wq2i2Wyyt0yQZqAoCkzTxM7ODt6+fRvYXjQa9eSWAXcC+ZC8HV3Xpyr5jcfjhRGywJ1QhcNhVKtVjEYjVst4MBgsZdOSpLt6NltbW4zcbNuGpml48+bNXFuI397yuWtKruuiVquxImYUQzPruWKxGCuFS9fLsoxGo4FQKMReNrDoGJfP5z2xZWTfWmb+yOM1S6siD3MikcD19fXUs7TbbZaSdJ+YNgBMTviTApFjKpVidcbo2cfjMRqNxq/PpgSA5XiRYZbUxj//+c/493//96mHpqPFZDLB+fk5bm5uEA6H2aRbloWLiwtPHWNN0/D48WP0er3AwK7Dw0MWd0H343e2VCqFeDzueQ1S0AKXJAnJZJIVZQfAAuj83g2qTsiryI7j4Pz8fMpbyBufyU27s7OD//zP//S0uba2hlwuN5Vf1el0FkaAU/VN/p6fM1zXRalUgqZpWF9fZ0X4Zi2iSCTC5IUWNOVgGoYBwzBY/BKlgrx69crTnizLKBaLntIdruvO1JJojHVdx8bGBtvQgo6bPFlks1mcnJxMVTWgqgJ0rSzLODs7YxoYcGfH9c819d1vw8rlcqhUKkilUp5cPkmSkM/n8be//W3eFPxi+GBSosqS6XSa5c0oioJ2u80qLPb7faTTaQyHQxQKBZimycqadDqdqeNMu93Gzc0NisUiSz2hVxwlEgkMBgOkUikMBgNks1lWEhe4m4xKpeIhL03T2IsJgLsd7dmzZ6jVah7XKKWWkO2HvFdUnpfaJ+/e1GByUbu6riMajTI7F7+LK4qCXq/HxqfX6yGZTOLg4MBTqI0E7YcffmDCZJomi33h4a/Rw++wfLQxvWKnVCqt5C7Jg0ieYmuovzs7OxiPx2i1WuwtH5SDSeNCWhIFNL58+RJHR0eIRqOQZZmV2zk9PWWvVYrH41NBqL1ezyOfqqqyt+CkUimmGdObSfyERBs1/Z9/88zLly8RCoVwe3uLXC43Vfp3MpmgXq/Dde8qshYKBTiOg8FggKurK1iWhU6nw7y1/jCS7777Du12G3t7e4wU6WUH//Vf//XxJ+wj4aNYuTqdDq6urrCxscGOHZPJBF9//TVkWUatVsP+/j4r6SBJdxUgc7kcjo+PA6s+np6eYjQaed4MQS9pbLfb2N/fZ6+fabVaLN6i3W5PHbUovYTO0+1221OPmSLR6f1sZAsio7e/BCkJH2Vr0ytv+D/kten1enj79i1yuZwnZUaWZfZKpFar5Ymx4pMjq9Wq5yUBT548YdoefRakFWmahmg0io2NDYTDYfbqHNLw+v3+z1q466Ho9Xo4OTnBwcEBgPdG3IODA4xGI8Tjceb+JlJ2HAcXFxeeetKdTgdv375lBfwpUn9vbw9bW1s4Pz+HruuelwjYto1KpeLZAKLRKA4ODthcDodD5tjgtSGCbduo1WpsU6LvVFXFN998g2w2ix9//HFmvS0ygVAwraIoiEaj+OMf/wjbtnFxccFsjrwc3N7eolwue4r/0+b06tWrlX7bzUchJcdxUCqVIMt3xd754uaTyQS5XM5T4J12oOPj45keLdd1WUwOaSf8H1pQfHIjgMCI2eFw6Inq5gXHH5C4KEXDMAxsbm4ikUiwlAD/GyokSWJvRz09PUW73cbFxQUKhYIngZeEbG1tbeq+VDqCN8a6rsuKumezWQDeuBt/ega9J43GiL9HMpn8LEgJuKvP9ebNG4xGI7aB0JEYeH/U4YkkyM3dbDZxfHyMw8NDppGGw2H2lhoe5HjxlwEhrd1vlPY7dihgslwuo91uMy0VeG/X0TQN1WqVvR2H2qc//DP0+31PqhFF6wdpdwDYRs9rX67rIhqNIh6Pf1AlhU+Nj+YPnEwmKJVKMAyDHeWA97YX4H09oXq9vtBwy0OWZWao44Mk6e9FrzB2XdfzehleYyFy4MmNvGWzyu6Gw2H2kkG6Py0MEpqrqyuPZ+P8/JyRNl8hk9qk5xkMBuj3+3j16lXga3vo5YJ8v4Niafi4MdJcZVlmNdRX0RU8D6QV89nx5JHlNzuq4jArrotqLR0eHnrqGPHahG3baDQaKJfLUzJKgaw0h2TfIdkh+apWq556RMfHxyxPDYDnqEn3Be7I7MWLF1NOotvbWxSLRc864F+IQDLEv4KbXs8NvNe+f/rpp5kOo1XBR5XM8XiMN2/eYH9/n0W0Al6DH+1W93FZkzaRSCSYi5h/u8Xl5eVSpUAAr9YFvA8noMxwsnHNUm/H4zFOT089eXs8SED9R1JKBbEsC8VikZHCZDJBo9FAq9XyvAwgCJQO02g02KvDI5FIYLAgcKc1NhoN9oopXqP7XFNRaM6DjkqDwQCVSmWhbN3e3jJtnup2J5NJpFIp5v0tl8uB4SSWZeHs7MxTaXXWPXh0Oh28efMGu7u7TMvmn4neKzgr2hsAyuWy54WokUgE6XQa8XgckvT+pQr1ep05k8bjMXq9HprNpicodZUhzTuqSJL0IEsovYmE3KO2bbOs5tFo9KC3ItDORLE7pmky+1WpVPqguBza3SKRCHvRwacyApPnQ9M0VruHbCL3bYeqJ8TjcYTDYaaVDodDdDod9Ho9j63lPnBd95O77x4qXxR3EwqFGLGQdvDQ/C3Sck3TZEelTwFaG7FYjL1XkOyWD5UBTdMQi8XQ6XRYiA6BckBXLURknnx9ElIS+PyxyqQk8Pljnnz9eqrNCwgI/CogSElAQGClIEhJQEBgpSBISUBAYKUgSElAQGClIEhJQEBgpSBISUBAYKUgSElAQGClIEhJQEBgpTA3oltAQEDg54bQlAQEBFYKgpQEBARWCoKUBAQEVgqClAQEBFYKgpQEBARWCoKUBAQEVgr/HwqSphzruY+lAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 360x216 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Path to the data directory\n",
    "# data_dir = Path(\"../input/captcha-version-2-images/samples/\")\n",
    "# data_dir = Path('captcha-version-2-images/samples')\n",
    "data_dir = Path('data/wetax')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get list of all the images\n",
    "images = list(data_dir.glob(\"*.png\"))\n",
    "print(\"Number of images found: \", len(images))\n",
    "\n",
    "\n",
    "# Let's take a look at some samples first. \n",
    "# Always look at your data!\n",
    "sample_images = images[:4]\n",
    "\n",
    "_,ax = plt.subplots(2,2, figsize=(5,3))\n",
    "for i in range(4):\n",
    "    img = cv2.imread(str(sample_images[i]))\n",
    "    print(\"Shape of image: \", img.shape)\n",
    "    ax[i//2, i%2].imshow(img)\n",
    "    ax[i//2, i%2].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unqiue charcaters in the whole dataset:  10\n",
      "Maximum length of any captcha:  6\n",
      "Characters present:  ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "Total number of samples in the dataset:  1104\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data\\wetax\\742580.png</td>\n",
       "      <td>742580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data\\wetax\\947669.png</td>\n",
       "      <td>947669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data\\wetax\\528270.png</td>\n",
       "      <td>528270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data\\wetax\\109967.png</td>\n",
       "      <td>109967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data\\wetax\\457438.png</td>\n",
       "      <td>457438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                img_path   label\n",
       "0  data\\wetax\\742580.png  742580\n",
       "1  data\\wetax\\947669.png  947669\n",
       "2  data\\wetax\\528270.png  528270\n",
       "3  data\\wetax\\109967.png  109967\n",
       "4  data\\wetax\\457438.png  457438"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store all the characters in a set\n",
    "characters = set()\n",
    "\n",
    "# A list to store the length of each captcha\n",
    "captcha_length = []\n",
    "\n",
    "# Store image-label info\n",
    "dataset = []\n",
    "\n",
    "# Iterate over the dataset and store the\n",
    "# information needed\n",
    "for img_path in images:\n",
    "    # 1. Get the label associated with each image\n",
    "    label = img_path.name.split(\".png\")[0]\n",
    "    # 2. Store the length of this cpatcha\n",
    "    captcha_length.append(len(label))\n",
    "    # 3. Store the image-label pair info\n",
    "    dataset.append((str(img_path), label))\n",
    "    \n",
    "    # 4. Store the characters present\n",
    "    for ch in label:\n",
    "        characters.add(ch)\n",
    "\n",
    "# Sort the characters        \n",
    "characters = sorted(characters)\n",
    "\n",
    "# Convert the dataset info into a dataframe\n",
    "dataset = pd.DataFrame(dataset, columns=[\"img_path\", \"label\"], index=None)\n",
    "\n",
    "# Shuffle the dataset\n",
    "dataset = dataset.sample(frac=1.).reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"Number of unqiue charcaters in the whole dataset: \", len(characters))\n",
    "print(\"Maximum length of any captcha: \", max(Counter(captcha_length).keys()))\n",
    "print(\"Characters present: \", characters)\n",
    "print(\"Total number of samples in the dataset: \", len(dataset))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:  993\n",
      "Number of validation samples:  111\n",
      "Number of training images:  (993, 50, 200)\n",
      "Number of training labels:  (993,)\n",
      "Number of validation images:  (111, 50, 200)\n",
      "Number of validation labels:  (111,)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "training_data, validation_data = train_test_split(dataset, test_size=0.1, random_state=seed)\n",
    "\n",
    "training_data = training_data.reset_index(drop=True)\n",
    "validation_data = validation_data.reset_index(drop=True)\n",
    "\n",
    "print(\"Number of training samples: \", len(training_data))\n",
    "print(\"Number of validation samples: \", len(validation_data))\n",
    "\n",
    "\n",
    "\n",
    "# Map text to numeric labels \n",
    "char_to_labels = {char:idx for idx, char in enumerate(characters)}\n",
    "\n",
    "# Map numeric labels to text\n",
    "labels_to_char = {val:key for key, val in char_to_labels.items()}\n",
    "\n",
    "\n",
    "\n",
    "# Sanity check for corrupted images\n",
    "def is_valid_captcha(captcha):\n",
    "    for ch in captcha:\n",
    "        if not ch in characters:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "# Store arrays in memory as it's not a muvh big dataset\n",
    "def generate_arrays(df, resize=True, img_height=50, img_width=200):\n",
    "    \"\"\"Generates image array and labels array from a dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df: dataframe from which we want to read the data\n",
    "        resize (bool)    : whether to resize images or not\n",
    "        img_weidth (int): width of the resized images\n",
    "        img_height (int): height of the resized images\n",
    "        \n",
    "    Returns:\n",
    "        images (ndarray): grayscale images\n",
    "        labels (ndarray): corresponding encoded labels\n",
    "    \"\"\"\n",
    "    \n",
    "    num_items = len(df)\n",
    "    images = np.zeros((num_items, img_height, img_width), dtype=np.float32)\n",
    "    labels = [0]*num_items\n",
    "    \n",
    "    for i in range(num_items):\n",
    "        img = cv2.imread(df[\"img_path\"][i])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        if resize: \n",
    "            img = cv2.resize(img, (img_width, img_height))\n",
    "        \n",
    "        img = (img/255.).astype(np.float32)\n",
    "        label = df[\"label\"][i]\n",
    "        \n",
    "        # Add only if it is a valid captcha\n",
    "        if is_valid_captcha(label):\n",
    "            images[i, :, :] = img\n",
    "            labels[i] = label\n",
    "    \n",
    "    return images, np.array(labels)\n",
    "\n",
    "\n",
    "\n",
    "# Build training data\n",
    "training_data, training_labels = generate_arrays(df=training_data)\n",
    "print(\"Number of training images: \", training_data.shape)\n",
    "print(\"Number of training labels: \", training_labels.shape)\n",
    "\n",
    "\n",
    "# Build validation data\n",
    "validation_data, validation_labels = generate_arrays(df=validation_data)\n",
    "print(\"Number of validation images: \", validation_data.shape)\n",
    "print(\"Number of validation labels: \", validation_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    \"\"\"Generates batches from a given dataset.\n",
    "    \n",
    "    Args:\n",
    "        data: training or validation data\n",
    "        labels: corresponding labels\n",
    "        char_map: dictionary mapping char to labels\n",
    "        batch_size: size of a single batch\n",
    "        img_width: width of the resized\n",
    "        img_height: height of the resized\n",
    "        downsample_factor: by what factor did the CNN downsample the images\n",
    "        max_length: maximum length of any captcha\n",
    "        shuffle: whether to shuffle data or not after each epoch\n",
    "    Returns:\n",
    "        batch_inputs: a dictionary containing batch inputs \n",
    "        batch_labels: a batch of corresponding labels \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 labels,\n",
    "                 char_map,\n",
    "                 batch_size=16,\n",
    "                 img_width=200,\n",
    "                 img_height=50,\n",
    "                 downsample_factor=4,\n",
    "                 max_length=6,\n",
    "                 shuffle=True\n",
    "                ):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.char_map = char_map\n",
    "        self.batch_size = batch_size\n",
    "        self.img_width = img_width\n",
    "        self.img_height = img_height\n",
    "        self.downsample_factor = downsample_factor\n",
    "        self.max_length = max_length\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(data))    \n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.data) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 1. Get the next batch indices\n",
    "        curr_batch_idx = self.indices[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "        \n",
    "        # 2. This isn't necessary but it can help us save some memory\n",
    "        # as not all batches the last batch may not have elements\n",
    "        # equal to the batch_size \n",
    "        batch_len = len(curr_batch_idx)\n",
    "        \n",
    "        # 3. Instantiate batch arrays\n",
    "        batch_images = np.ones((batch_len, self.img_width, self.img_height, 1),\n",
    "                               dtype=np.float32)\n",
    "        batch_labels = np.ones((batch_len, self.max_length), dtype=np.float32)\n",
    "        input_length = np.ones((batch_len, 1), dtype=np.int64) * \\\n",
    "                                (self.img_width // self.downsample_factor - 2)\n",
    "        label_length = np.zeros((batch_len, 1), dtype=np.int64)\n",
    "        \n",
    "        \n",
    "        for j, idx in enumerate(curr_batch_idx):\n",
    "            # 1. Get the image and transpose it\n",
    "            img = self.data[idx].T\n",
    "            # 2. Add extra dimenison\n",
    "            img = np.expand_dims(img, axis=-1)\n",
    "            # 3. Get the correpsonding label\n",
    "            text = self.labels[idx]\n",
    "            # 4. Include the pair only if the captcha is valid\n",
    "            if is_valid_captcha(text):\n",
    "                label = [self.char_map[ch] for ch in text]\n",
    "                batch_images[j] = img\n",
    "                batch_labels[j] = label\n",
    "                label_length[j] = len(text)\n",
    "        \n",
    "        batch_inputs = {\n",
    "                'input_data': batch_images,\n",
    "                'input_label': batch_labels,\n",
    "                'input_length': input_length,\n",
    "                'label_length': label_length,\n",
    "                }\n",
    "        return batch_inputs, np.zeros(batch_len).astype(np.float32)\n",
    "        \n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Batch size for training and validation\n",
    "batch_size = 16\n",
    "\n",
    "# Desired image dimensions\n",
    "img_width=200\n",
    "img_height=50 \n",
    "\n",
    "# Factor  by which the image is going to be downsampled\n",
    "# by the convolutional blocks\n",
    "downsample_factor=4\n",
    "\n",
    "# Maximum length of any captcha in the data\n",
    "max_length=6\n",
    "\n",
    "# Get a generator object for the training data\n",
    "train_data_generator = DataGenerator(data=training_data,\n",
    "                                     labels=training_labels,\n",
    "                                     char_map=char_to_labels,\n",
    "                                     batch_size=batch_size,\n",
    "                                     img_width=img_width,\n",
    "                                     img_height=img_height,\n",
    "                                     downsample_factor=downsample_factor,\n",
    "                                     max_length=max_length,\n",
    "                                     shuffle=True\n",
    "                                    )\n",
    "\n",
    "# Get a generator object for the validation data \n",
    "valid_data_generator = DataGenerator(data=validation_data,\n",
    "                                     labels=validation_labels,\n",
    "                                     char_map=char_to_labels,\n",
    "                                     batch_size=batch_size,\n",
    "                                     img_width=img_width,\n",
    "                                     img_height=img_height,\n",
    "                                     downsample_factor=downsample_factor,\n",
    "                                     max_length=max_length,\n",
    "                                     shuffle=False\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CTCLayer(layers.Layer):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.loss_fn = keras.backend.ctc_batch_cost\n",
    "\n",
    "    def call(self, y_true, y_pred, input_length, label_length):\n",
    "        # Compute the training-time loss value and add it\n",
    "        # to the layer using `self.add_loss()`.\n",
    "        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n",
    "        self.add_loss(loss)\n",
    "        \n",
    "        # On test time, just return the computed loss\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    # Inputs to the model\n",
    "    input_img = layers.Input(shape=(img_width, img_height, 1),\n",
    "                            name='input_data',\n",
    "                            dtype='float32')\n",
    "    labels = layers.Input(name='input_label', shape=[max_length], dtype='float32')\n",
    "    input_length = layers.Input(name='input_length', shape=[1], dtype='int64')\n",
    "    label_length = layers.Input(name='label_length', shape=[1], dtype='int64')\n",
    "    \n",
    "    # First conv block\n",
    "    x = layers.Conv2D(32,\n",
    "               (3,3),\n",
    "               activation='relu',\n",
    "               kernel_initializer='he_normal',\n",
    "               padding='same',\n",
    "               name='Conv1')(input_img)\n",
    "    x = layers.MaxPooling2D((2,2), name='pool1')(x)\n",
    "    \n",
    "    # Second conv block\n",
    "    x = layers.Conv2D(64,\n",
    "               (3,3),\n",
    "               activation='relu',\n",
    "               kernel_initializer='he_normal',\n",
    "               padding='same',\n",
    "               name='Conv2')(x)\n",
    "    x = layers.MaxPooling2D((2,2), name='pool2')(x)\n",
    "    \n",
    "    # We have used two max pool with pool size and strides of 2.\n",
    "    # Hence, downsampled feature maps are 4x smaller. The number of\n",
    "    # filters in the last layer is 64. Reshape accordingly before\n",
    "    # passing it to RNNs\n",
    "    new_shape = ((img_width // 4), (img_height // 4)*64)\n",
    "    x = layers.Reshape(target_shape=new_shape, name='reshape')(x)\n",
    "    x = layers.Dense(64, activation='relu', name='dense1')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # RNNs\n",
    "    x = layers.Bidirectional(layers.LSTM(128,\n",
    "                                         return_sequences=True,\n",
    "                                         dropout=0.2))(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(64,\n",
    "                                         return_sequences=True,\n",
    "                                         dropout=0.25))(x)\n",
    "    \n",
    "    # Predictions\n",
    "    x = layers.Dense(len(characters)+1,\n",
    "              activation='softmax', \n",
    "              name='dense2',\n",
    "              kernel_initializer='he_normal')(x)\n",
    "    \n",
    "    # Calculate CTC\n",
    "    output = CTCLayer(name='ctc_loss')(labels, x, input_length, label_length)\n",
    "    \n",
    "    # Define the model\n",
    "    model = keras.models.Model(inputs=[input_img,\n",
    "                                       labels,\n",
    "                                       input_length,\n",
    "                                       label_length],\n",
    "                                outputs=output,\n",
    "                                name='ocr_model_v1')\n",
    "    \n",
    "    # Optimizer\n",
    "    sgd = keras.optimizers.SGD(learning_rate=0.002,\n",
    "                               decay=1e-6,\n",
    "                               momentum=0.9,\n",
    "                               nesterov=True,\n",
    "                               clipnorm=5)\n",
    "    \n",
    "    # Compile the model and return \n",
    "    model.compile(optimizer=sgd)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ocr_model_v1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_data (InputLayer)         [(None, 200, 50, 1)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv1 (Conv2D)                  (None, 200, 50, 32)  320         input_data[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)            (None, 100, 25, 32)  0           Conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Conv2 (Conv2D)                  (None, 100, 25, 64)  18496       pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pool2 (MaxPooling2D)            (None, 50, 12, 64)   0           Conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 50, 768)      0           pool2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 50, 64)       49216       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 50, 64)       0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 50, 256)      197632      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 50, 128)      164352      bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_label (InputLayer)        [(None, 6)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 50, 11)       1419        bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_length (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "label_length (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ctc_loss (CTCLayer)             (None, 1)            0           input_label[0][0]                \n",
      "                                                                 dense2[0][0]                     \n",
      "                                                                 input_length[0][0]               \n",
      "                                                                 label_length[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 431,435\n",
      "Trainable params: 431,435\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "63/63 [==============================] - 5s 73ms/step - loss: 19.5224 - val_loss: 14.7701\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 2s 39ms/step - loss: 13.0582 - val_loss: 10.6795\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 2s 40ms/step - loss: 9.1407 - val_loss: 6.5482\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 2s 39ms/step - loss: 4.8769 - val_loss: 2.1413\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 2s 39ms/step - loss: 1.5388 - val_loss: 0.4678\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 2s 39ms/step - loss: 0.5461 - val_loss: 0.3610\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 2s 39ms/step - loss: 0.3373 - val_loss: 0.3925\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 3s 40ms/step - loss: 0.2744 - val_loss: 0.3926\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 3s 41ms/step - loss: 0.2487 - val_loss: 0.4832\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 2s 39ms/step - loss: 0.2170 - val_loss: 0.4969\n"
     ]
    }
   ],
   "source": [
    "# Add early stopping\n",
    "es = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                   patience=5,\n",
    "                                   restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_data_generator,\n",
    "                    validation_data=valid_data_generator,\n",
    "                    epochs=10,\n",
    "                    callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_data (InputLayer)      [(None, 200, 50, 1)]      0         \n",
      "_________________________________________________________________\n",
      "Conv1 (Conv2D)               (None, 200, 50, 32)       320       \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling2D)         (None, 100, 25, 32)       0         \n",
      "_________________________________________________________________\n",
      "Conv2 (Conv2D)               (None, 100, 25, 64)       18496     \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling2D)         (None, 50, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 50, 768)           0         \n",
      "_________________________________________________________________\n",
      "dense1 (Dense)               (None, 50, 64)            49216     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 50, 64)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 50, 256)           197632    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 50, 128)           164352    \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 50, 11)            1419      \n",
      "=================================================================\n",
      "Total params: 431,435\n",
      "Trainable params: 431,435\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "prediction_model = keras.models.Model(model.get_layer(name='input_data').input,\n",
    "                                        model.get_layer(name='dense2').output)\n",
    "prediction_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A utility to decode the output of the network\n",
    "def decode_batch_predictions(pred):\n",
    "    pred = pred[:, :-2]\n",
    "    input_len = np.ones(pred.shape[0])*pred.shape[1]\n",
    "    \n",
    "    # Use greedy search. For complex tasks, you can use beam search\n",
    "    results = keras.backend.ctc_decode(pred, \n",
    "                                        input_length=input_len,\n",
    "                                        greedy=True)[0][0]\n",
    "    \n",
    "    # Iterate over the results and get back the text\n",
    "    output_text = []\n",
    "    for res in results.numpy():\n",
    "        outstr = ''\n",
    "        for c in res:\n",
    "            if c < len(characters) and c >=0:\n",
    "                outstr += labels_to_char[c]\n",
    "        output_text.append(outstr)\n",
    "    \n",
    "    # return final text results\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\arkwith\\captcha\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "Ground truth: 906692 \t Predicted: 906692\n",
      "Ground truth: 242415 \t Predicted: 242415\n",
      "Ground truth: 318740 \t Predicted: 318740\n",
      "Ground truth: 677565 \t Predicted: 677565\n",
      "Ground truth: 029523 \t Predicted: 029523\n",
      "Ground truth: 271338 \t Predicted: 271338\n",
      "Ground truth: 870904 \t Predicted: 870904\n",
      "Ground truth: 095452 \t Predicted: 095452\n",
      "Ground truth: 031757 \t Predicted: 031757\n",
      "Ground truth: 930335 \t Predicted: 930335\n",
      "Ground truth: 897002 \t Predicted: 897002\n",
      "Ground truth: 494513 \t Predicted: 494513\n",
      "Ground truth: 949868 \t Predicted: 948968\n",
      "Ground truth: 215456 \t Predicted: 215456\n",
      "Ground truth: 711302 \t Predicted: 711302\n",
      "Ground truth: 875023 \t Predicted: 875023\n"
     ]
    }
   ],
   "source": [
    "#  Let's check results on some validation samples\n",
    "for p, (inp_value, _) in enumerate(valid_data_generator):\n",
    "    bs = inp_value['input_data'].shape[0]\n",
    "    X_data = inp_value['input_data']\n",
    "    labels = inp_value['input_label']\n",
    "    \n",
    "    preds = prediction_model.predict(X_data)\n",
    "    pred_texts = decode_batch_predictions(preds)\n",
    "    \n",
    "    \n",
    "    orig_texts = []\n",
    "    for label in labels:\n",
    "        text = ''.join([labels_to_char[int(x)] for x in label])\n",
    "        orig_texts.append(text)\n",
    "        \n",
    "    for i in range(bs):\n",
    "        print(f'Ground truth: {orig_texts[i]} \\t Predicted: {pred_texts[i]}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define function to predict captcha\n",
    "def predict(filepath):\n",
    "    resize=True\n",
    "    img = cv2.imread(filepath)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    if resize:\n",
    "        img = cv2.resize(img, (50, 200))\n",
    "        img = (img/255.).astype(np.float32)\n",
    "\n",
    "    preds = prediction_model.predict(img)\n",
    "    pred_texts = decode_batch_predictions(preds)\n",
    "\n",
    "    return pred_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 200, 50, 1) for input Tensor(\"input_data:0\", shape=(None, 200, 50, 1), dtype=float32), but it was called on an input with incompatible shape (None, 50).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    D:\\arkwith\\captcha\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1462 predict_function  *\n        return step_function(self, iterator)\n    D:\\arkwith\\captcha\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1452 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    D:\\arkwith\\captcha\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    D:\\arkwith\\captcha\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    D:\\arkwith\\captcha\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    D:\\arkwith\\captcha\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1445 run_step  **\n        outputs = model.predict_step(data)\n    D:\\arkwith\\captcha\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1418 predict_step\n        return self(x, training=False)\n    D:\\arkwith\\captcha\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:985 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    D:\\arkwith\\captcha\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:386 call\n        inputs, training=training, mask=mask)\n    D:\\arkwith\\captcha\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:508 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    D:\\arkwith\\captcha\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:976 __call__\n        self.name)\n    D:\\arkwith\\captcha\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:196 assert_input_compatibility\n        str(x.shape.as_list()))\n\n    ValueError: Input 0 of layer Conv1 is incompatible with the layer: : expected min_ndim=4, found ndim=2. Full shape received: [None, 50]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-cd6ce4bbff8a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/test/wetax/028885.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-1a10525eb9f4>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(filepath)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m255.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprediction_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mpred_texts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecode_batch_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\arkwith\\captcha\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[0;32m    129\u001b[0m           method.__name__))\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[1;32mD:\\arkwith\\captcha\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1597\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1598\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1599\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1600\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1601\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\arkwith\\captcha\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\arkwith\\captcha\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    812\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32mD:\\arkwith\\captcha\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2826\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2828\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2829\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\arkwith\\captcha\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3208\u001b[0m           \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3209\u001b[0m           and call_context_key in self._function_cache.missed):\n\u001b[1;32m-> 3210\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_define_function_with_shape_relaxation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\arkwith\\captcha\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3141\u001b[0m     graph_function = self._create_graph_function(\n\u001b[1;32m-> 3142\u001b[1;33m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[0;32m   3143\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\arkwith\\captcha\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3075\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\arkwith\\captcha\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\arkwith\\captcha\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\arkwith\\captcha\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    D:\\arkwith\\captcha\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1462 predict_function  *\n        return step_function(self, iterator)\n    D:\\arkwith\\captcha\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1452 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    D:\\arkwith\\captcha\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    D:\\arkwith\\captcha\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    D:\\arkwith\\captcha\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    D:\\arkwith\\captcha\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1445 run_step  **\n        outputs = model.predict_step(data)\n    D:\\arkwith\\captcha\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1418 predict_step\n        return self(x, training=False)\n    D:\\arkwith\\captcha\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:985 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    D:\\arkwith\\captcha\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:386 call\n        inputs, training=training, mask=mask)\n    D:\\arkwith\\captcha\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:508 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    D:\\arkwith\\captcha\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:976 __call__\n        self.name)\n    D:\\arkwith\\captcha\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:196 assert_input_compatibility\n        str(x.shape.as_list()))\n\n    ValueError: Input 0 of layer Conv1 is incompatible with the layer: : expected min_ndim=4, found ndim=2. Full shape received: [None, 50]\n"
     ]
    }
   ],
   "source": [
    "print(predict('data/test/wetax/028885.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
